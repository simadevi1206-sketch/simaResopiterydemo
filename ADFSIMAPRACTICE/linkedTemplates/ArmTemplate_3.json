{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "ADFSIMAPRACTICE"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/DataFlow3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Real Time Scenario Data"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSql",
								"type": "DatasetReference"
							},
							"name": "source"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink"
						}
					],
					"transformations": [
						{
							"name": "filter"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SalesOrderID as integer,",
						"          SalesOrderDetailID as integer,",
						"          OrderQty as short,",
						"          ProductID as integer,",
						"          UnitPrice as decimal(19,4),",
						"          UnitPriceDiscount as decimal(19,4),",
						"          LineTotal as decimal(38,6),",
						"          rowguid as string,",
						"          ModifiedDate as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table',",
						"     partitionBy('hash', 1)) ~> source",
						"source filter(UnitPrice<500,",
						"     partitionBy('hash', 1)) ~> filter",
						"filter sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Error Handling Dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Real Time Scenario Data"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "sink2",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "conditionalsplit1"
						},
						{
							"name": "derivedColumnforErrorRows"
						},
						{
							"name": "derivedforGoodRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as string,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"employee split(isNull(toDate(DateOfJoining)),",
						"     disjoint: false,",
						"     partitionBy('hash', 1)) ~> conditionalsplit1@(errorRows, GoodRows)",
						"conditionalsplit1@errorRows derive(FileName = \"employee_data.txt\") ~> derivedColumnforErrorRows",
						"conditionalsplit1@GoodRows derive(fileName = \"employee_data.txt\",",
						"     partitionBy('hash', 1)) ~> derivedforGoodRows",
						"derivedColumnforErrorRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as string,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink",
						"derivedforGoodRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as string,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Exist dataFlow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "employee1csvsamebutfewextrarowadd",
								"type": "DatasetReference"
							},
							"name": "Employee"
						},
						{
							"dataset": {
								"referenceName": "joinsinputdepartment",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "exists"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> Employee",
						"source(output(",
						"          DepartmentID as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> department",
						"Employee, department exists(employeeID == department@DepartmentID,",
						"     negate:false,",
						"     partitionBy('hash', 1),",
						"     broadcast: 'auto')~> exists",
						"exists sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Exist.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Lookup_Dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "employee1csvsamebutfewextrarowadd",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "joinsinputdepartment",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "lookup"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          DepartmentID as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee, department lookup(employee@DepartmentID == department@DepartmentID,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     partitionBy('hash', 1),",
						"     broadcast: 'auto')~> lookup",
						"lookup sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['LookupDataflow_csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/NotExist')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "employee1csvsamebutfewextrarowadd",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "joinsinputdepartment",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "exists"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          DepartmentID as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee, department exists(employee@DepartmentID == department@DepartmentID,",
						"     negate:true,",
						"     partitionBy('hash', 1),",
						"     broadcast: 'auto')~> exists",
						"exists sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['NotExist.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Name,",
						"          DepartmentID,",
						"          DateOfJoining,",
						"          employeeID",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ParameterDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "Sink"
						}
					],
					"transformations": [
						{
							"name": "filter"
						}
					],
					"scriptLines": [
						"parameters{",
						"     deptName as string",
						"}",
						"source(output(",
						"          Name as string,",
						"          DepartmentID as integer,",
						"          DateOfJoining as date,",
						"          employeeID as integer,",
						"          Salary as integer,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> employee",
						"employee filter(DepartmentName==$deptName,",
						"     partitionBy('hash', 1)) ~> filter",
						"filter sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ParameterDataFlow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Name,",
						"          DepartmentID,",
						"          DateOfJoining,",
						"          employeeID,",
						"          Salary,",
						"          DepartmentName",
						"     ),",
						"     partitionBy('hash', 1)) ~> Sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RankDataFlow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "rank"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> employee",
						"employee rank(asc(Salary, true),",
						"     output(Ranking as long),",
						"     partitionBy('hash', 1)) ~> rank",
						"rank sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['RankingDataflow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RunningTotal')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Real Time Scenario Data"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Runningtotal",
								"type": "DatasetReference"
							},
							"name": "employeerunning"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "window"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmployeeID as short,",
						"          EmployeeName as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          Salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employeerunning",
						"employeerunning window(asc(Salary, true),",
						"     {Running Total} = sum(toInteger(Salary)),",
						"     partitionBy('hash', 1)) ~> window",
						"window sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SelectDataFlow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "source"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "select"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source",
						"source select(mapColumn(",
						"          Name,",
						"          DepartmentID,",
						"          DateOfJoining,",
						"          employeeID",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select",
						"select sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SelectDataflow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Surrogate')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "joinsinputdepartment",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "surrogateKey"
						}
					],
					"scriptLines": [
						"source(output(",
						"          DepartmentID as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"department keyGenerate(output(DepartmentKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L,",
						"     partitionBy('hash', 1)) ~> surrogateKey",
						"surrogateKey sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Union')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "UnionHR",
								"type": "DatasetReference"
							},
							"name": "HR"
						},
						{
							"dataset": {
								"referenceName": "UnionIT",
								"type": "DatasetReference"
							},
							"name": "IT"
						},
						{
							"dataset": {
								"referenceName": "UnionPayRoll",
								"type": "DatasetReference"
							},
							"name": "PayRoll"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "union"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> HR",
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> IT",
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> PayRoll",
						"HR, IT, PayRoll union(byName: true,",
						"     partitionBy('hash', 1))~> union",
						"union sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Union.Dataflow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/WindowDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "window"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as integer,",
						"          DateOfJoining as date,",
						"          employeeID as integer,",
						"          Salary as integer,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> employee",
						"employee window(over(DepartmentName),",
						"     asc(Salary, true),",
						"     AvgSalary = avg(Salary),",
						"     partitionBy('hash', 1)) ~> window",
						"window sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['WindowFunction.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/conditionaldf')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "Conditionalsplit"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "HRsinkCondionalSplit",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "ITsinkCondionalSplit",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "PayrollsinkCondionalSplit",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "OthersinkCondionalSplit",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "splitCondition"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Conditionalsplit",
						"Conditionalsplit split(equals(DepartmentID,'1'),",
						"     equals(DepartmentID,'2'),",
						"     equals(DepartmentID,'3'),",
						"     disjoint: false,",
						"     partitionBy('hash', 1)) ~> splitCondition@(HRemployees, ITemployees, Payrollemployees, otheremployee)",
						"splitCondition@HRemployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ConditionalHR.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> HRsinkCondionalSplit",
						"splitCondition@ITemployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ITCondionalSplit.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> ITsinkCondionalSplit",
						"splitCondition@Payrollemployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['PayrollsinkCondionalSplit.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> PayrollsinkCondionalSplit",
						"splitCondition@otheremployee sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['OthersinkCondionalSplit'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> OthersinkCondionalSplit"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink2"
						},
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink3"
						}
					],
					"transformations": [
						{
							"name": "Hr"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as short,",
						"          DateOfJoining as string,",
						"          employeeID as short,",
						"          Salary as short,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> source1",
						"source1 split(DepartmentName == 'HR',",
						"     DepartmentName == 'IT',",
						"     disjoint: false,",
						"     partitionBy('hash', 1)) ~> Hr@(Hr, IT, Other)",
						"Hr@Hr sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1",
						"Hr@IT sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2",
						"Hr@Other sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink3"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Real Time Scenario Data"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filter1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as string,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 filter(toInteger(Salary) > 3000,",
						"     partitionBy('hash', 1)) ~> filter1",
						"filter1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['practice.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/filterdataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "joininputemployee",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "filterdataflowDepartmentsink",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "condition1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          Department as string,",
						"          DateOfJoining as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees filter(equals(DepartmentID,'3')) ~> condition1",
						"condition1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['FilterDataFlow'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Name,",
						"          DepartmentID,",
						"          Department",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/join')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "joininputemployee",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "joinsinputdepartment",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "joinsdatasetforsink",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "join"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          Department as string,",
						"          DateOfJoining as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          DepartmentID as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee, department join(employee@DepartmentID == department@DepartmentID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join",
						"join sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Name,",
						"          DepartmentID = employee@DepartmentID,",
						"          Department,",
						"          DateOfJoining,",
						"          DepartmentID = department@DepartmentID,",
						"          DepartmentName",
						"     )) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/powerquery')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "inputDatasetForAggregation",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> inputDatasetForAggregation",
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared inputDatasetForAggregation = let AdfDoc = AzureStorage.BlobContents(\"https://practiceaccountstorage.blob.core.windows.net/inputorsourcecontainer/employee1/employee_datahr.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"inputDatasetForAggregation\" in Source;\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/practice')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlOutput",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filter"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as string,",
						"          employeeID as string,",
						"          Salary as string,",
						"          DepartmentName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 filter(toDate(DateOfJoining)>toDate('2020-01-01'),",
						"     partitionBy('hash', 1)) ~> filter",
						"filter sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/sort_DataFlow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "inputDatasetForAggregation",
								"type": "DatasetReference"
							},
							"name": "Short"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Sinksource",
								"type": "DatasetReference"
							},
							"name": "sink",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "sort"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          DepartmentID as string,",
						"          DateOfJoining as date,",
						"          employeeID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> Short",
						"Short sort(asc(Name, true),",
						"     partitionBy('hash', 1)) ~> sort",
						"sort sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SortDataFlow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		}
	]
}